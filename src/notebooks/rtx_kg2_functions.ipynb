{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94db8d0f-2dba-41af-a860-fcbe60c1824b",
   "metadata": {},
   "source": [
    "# RTX-KG2 Functions\n",
    "\n",
    "Convenience notebook for using functions from a single location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e598413e-509b-48b9-aab3-1c703b364c09",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pathlib\n",
    "import shutil\n",
    "from typing import Any, Dict, Generator, List, Literal\n",
    "\n",
    "import ijson\n",
    "import kuzu\n",
    "import requests\n",
    "from genson import SchemaBuilder\n",
    "from pyarrow import parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cc54d9-df5c-410d-97ea-603e493e42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, download_dir):\n",
    "    # referenced with modification from:\n",
    "    # https://stackoverflow.com/a/16696317\n",
    "    local_filename = url.split(\"/\")[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(f\"{download_dir}/{local_filename}\", \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                # if chunk:\n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced4dcc-a296-4df2-bfe5-4a2777940493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_gz(\n",
    "    tar_gz_path: str, output_dir: str, remove_tar_gz_after_extract: bool = True\n",
    "):\n",
    "\n",
    "    # Extract the tar.gz file\n",
    "    print(\"Extracting tar gz.\")\n",
    "    with tarfile.open(tar_gz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(output_dir)\n",
    "\n",
    "    if remove_tar_gz_after_extract:\n",
    "        print(\"Removing source tar gz file.\")\n",
    "        # Remove the temporary tar.gz file\n",
    "        pathlib.Path(tar_gz_path).unlink()\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b4285f-241c-4317-baa0-b0c4949710a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_level_names(json_file: str) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Find the topmost item names by way of streaming a json\n",
    "    file through ijson.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        parser = ijson.parse(f)\n",
    "        depth = 0\n",
    "        for prefix, event, value in parser:\n",
    "            if event == \"start_map\":\n",
    "                depth += 1\n",
    "            elif event == \"end_map\":\n",
    "                depth -= 1\n",
    "            elif event == \"map_key\" and depth == 1:\n",
    "                yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7de4e-cc65-45eb-9c12-9cfb4847c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_items_under_top_level_name(json_file: str, top_level_name: str):\n",
    "    \"\"\"\n",
    "    Count items under a top level object name\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(json_file, \"rb\") as f:\n",
    "        parser = ijson.items(f, f\"{top_level_name}.item\")\n",
    "        for item in parser:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ded656-e618-4956-9eeb-4ec6651c22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_items_by_topmost_item_name(\n",
    "    json_file: str, topmost_item_name: str, chunk_size: int, limit: int = 0\n",
    ") -> Generator[List[Dict[str, Any]], None, None]:\n",
    "    \"\"\"\n",
    "    Parse items using a topmost object name.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        objects = ijson.items(f, f\"{topmost_item_name}.item\")\n",
    "        chunk = []\n",
    "        limit_count = 0\n",
    "        for item in objects:\n",
    "            if limit == 0 or limit_count < limit:\n",
    "                chunk.append(item)\n",
    "                if len(chunk) == chunk_size:\n",
    "                    yield chunk\n",
    "                    limit_count += 1\n",
    "                    chunk = []\n",
    "        # Yield the last chunk if there are remaining elements\n",
    "        if chunk:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "895f978c-dd54-4eeb-b082-f8d50db3eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata_by_object_name(\n",
    "    json_file: str, metadata_object_name: str\n",
    ") -> Generator[Any, None, None]:\n",
    "    \"\"\"\n",
    "    Extract single value metadata from json file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        return next(ijson.items(f, metadata_object_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2b3a8-e6d9-4eb9-a7e7-2249b4bb6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cypher_table_create_stmt_from_parquet_file(\n",
    "    parquet_file: str,\n",
    "    table_type: Literal[\"node\", \"rel\"],\n",
    "    table_name: str,\n",
    "    table_pkey_parquet_field_name: str = \"id\",\n",
    "    rel_table_field_mapping: Dict[str, str] = {\"from\": \"nodes\", \"to\": \"nodes\"},\n",
    "    # specify a map for from to specification\n",
    "    # to move these to first two cols of related table\n",
    "    edges_to_or_from_fieldnames: List[str] = [\"subject\", \"object\"],\n",
    "):\n",
    "\n",
    "    parquet_schema = parquet.read_schema(parquet_file)\n",
    "\n",
    "    if table_pkey_parquet_field_name not in [field.name for field in parquet_schema]:\n",
    "        raise LookupError(\n",
    "            f\"Unable to find field {table_pkey_parquet_field_name} in parquet file {parquet_file}.\"\n",
    "        )\n",
    "\n",
    "    # Map Parquet data types to Cypher data types\n",
    "    # more details here: https://kuzudb.com/docusaurus/cypher/data-types/\n",
    "    parquet_to_cypher_type_mapping = {\n",
    "        \"string\": \"STRING\",\n",
    "        \"int32\": \"INT32\",\n",
    "        \"int64\": \"INT64\",\n",
    "        \"number\": \"FLOAT\",\n",
    "        \"float\": \"FLOAT\",\n",
    "        \"double\": \"FLOAT\",\n",
    "        \"boolean\": \"BOOLEAN\",\n",
    "        \"object\": \"MAP\",\n",
    "        \"array\": \"INT64[]\",\n",
    "        \"list<element: string>\": \"STRING[]\",\n",
    "        \"null\": \"NULL\",\n",
    "        \"date\": \"DATE\",\n",
    "        \"time\": \"TIME\",\n",
    "        \"datetime\": \"DATETIME\",\n",
    "        \"timestamp\": \"DATETIME\",\n",
    "        \"any\": \"ANY\",\n",
    "    }\n",
    "\n",
    "    # Generate Cypher field type statements\n",
    "    cypher_fields_from_parquet_schema = \", \".join(\n",
    "        [\n",
    "            # note: we use string splitting here for nested types\n",
    "            # for ex. list<element: string>\n",
    "            f\"{field.name} {parquet_to_cypher_type_mapping.get(str(field.type))}\"\n",
    "            for idx, field in enumerate(parquet_schema)\n",
    "            if table_type == \"node\" or (table_type == \"rel\" and idx > 1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # branch for creating node table\n",
    "    if table_type == \"node\":\n",
    "        return (\n",
    "            f\"CREATE NODE TABLE {table_name}\"\n",
    "            f\"({cypher_fields_from_parquet_schema}, \"\n",
    "            f\"PRIMARY KEY ({table_pkey_parquet_field_name}))\"\n",
    "        )\n",
    "\n",
    "    # else we return for rel tables\n",
    "    return (\n",
    "        f\"CREATE REL TABLE {table_name}\"\n",
    "        f\"(FROM {rel_table_field_mapping['from']} TO {rel_table_field_mapping['to']}, \"\n",
    "        f\"{cypher_fields_from_parquet_schema})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53059d44-e983-423d-9915-642c2e2f3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table_if_exists(kz_conn: kuzu.connection.Connection, table_name: str):\n",
    "    try:\n",
    "        kz_conn.execute(f\"DROP TABLE {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Warning: no need to drop table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16132bc-1acd-495c-b6b4-939ddb4a4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_table_names_from_parquet_path(\n",
    "    parquet_path: str,\n",
    "    column_with_table_name: str = \"id\",\n",
    "):\n",
    "    # return distinct table types as set comprehension\n",
    "    return set(\n",
    "        # create a parquet dataset and read a single column as an array\n",
    "        parquet.ParquetDataset(parquet_path)\n",
    "        .read(columns=[column_with_table_name])[column_with_table_name]\n",
    "        .to_pylist()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5b0ca-6140-4ec6-9a03-accf2fa6d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kz_execute_with_retries(\n",
    "    kz_conn: kuzu.connection.Connection, kz_stmt: str, retry_count: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Retry running a kuzu execution up to retry_count number of times.\n",
    "    \"\"\"\n",
    "\n",
    "    while retry_count > 1:\n",
    "\n",
    "        try:\n",
    "            kz_conn.execute(kz_stmt)\n",
    "            break\n",
    "        except RuntimeError as runexc:\n",
    "            # catch previous copy work and immediately move on\n",
    "            if (\n",
    "                str(runexc)\n",
    "                == \"Copy exception: COPY commands can only be executed once on a table.\"\n",
    "            ):\n",
    "                print(runexc)\n",
    "                break\n",
    "            elif \"Unable to find primary key value\" in str(runexc):\n",
    "                print(f\"Retrying after primary key exception: {runexc}\")\n",
    "                # wait a half second before attempting again\n",
    "                time.sleep(0.5)\n",
    "                retry_count -= 1\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a1a45-968c-44ac-8533-bc9ccbc81920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_table_names_from_parquet_path(\n",
    "    parquet_path: str,\n",
    "    column_with_table_name: str = \"id\",\n",
    "):\n",
    "    with duckdb.connect() as ddb:\n",
    "        return [\n",
    "            element[0]\n",
    "            for element in ddb.execute(\n",
    "                f\"\"\"\n",
    "            SELECT DISTINCT {column_with_table_name}\n",
    "            FROM read_parquet('{parquet_path}')\n",
    "            \"\"\"\n",
    "            ).fetchall()\n",
    "        ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
